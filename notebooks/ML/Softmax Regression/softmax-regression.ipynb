{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a43656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display, Markdown , Math \n",
    "\n",
    "sns.set()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f387a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string): display(Markdown(string))\n",
    "def latex(out): printmd(f'{out}')  \n",
    "def pr(string): printmd('***{}***'.format(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00947c5f",
   "metadata": {},
   "source": [
    "<h1> Softmax Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cbe18c",
   "metadata": {},
   "source": [
    "<h2>\n",
    "  <p>\n",
    "    <a href =   \"https://github.com/daodavid\" > \n",
    "         author: daodeiv (David Stankov) \n",
    "       <img src=\"https://cdn.thenewstack.io/media/2014/12/github-octocat.png\" align=\"left\" width=\"120\"  alt=\"daodavid\" ></a>\n",
    "    </p>      \n",
    "</h2>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b9fa4",
   "metadata": {},
   "source": [
    "<h6>\n",
    "  <font size=\"4\" face = \"Times New Roma\" color='#3f134f' > \n",
    "    <ul style=\"margin-left: 30px\">\n",
    "      <li><a href='#abstract'>Abstract </a> </li> <br>\n",
    "      <!--<li><a href='#int-1'>Introduction </a> </li><br> -->\n",
    "      <li><a href='#deff_softmax'>Softmaxt definition and  how it works?</a> </li><br>\n",
    "      <li><a href='#cross_entropy'>Cross-entropy Loss</a> </li><br>  \n",
    "      <li><a href='#optimization'>Optimization of softmax by Cross-entropy Loss and derivation of Gradient descent formula </a> </li><br>\n",
    "       <li><a href='#gradient'>Implementation of Gradient descent algorithm </a> </li><br> \n",
    "      <li><a href='#reg'>Regularization of gradient descent by learning rate and max iterations</a> </li><br>     \n",
    "       <li><a href='#conclusion'>Conclusion</a> </li><br>  \n",
    "        \n",
    "</ul>    \n",
    " </font>\n",
    "  </h6>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685bcee0",
   "metadata": {},
   "source": [
    "The Softmax function maybe is one of the most popular Machine learning algorithm basically, it turns arbitrary real values into probabilities, by using the exponential function. It could be considered a generalization of the sigmoid function. We could use softmax for multi-class classification furthermore it is met in many various fields of science as Statistical physics (Gibbs distributions), Quantum statistic, Information theory, and Neural networks. Softmax is much attractive in classification problems since it has a simple implementation and in many cases gives satisfying results and enough good performance.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff347bd4",
   "metadata": {},
   "source": [
    "We gonna use the Iris dataset because it is comparatively simple and  very convenient  in the studying Machine Learning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e396e401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.read_csv(\"../../../resources/data/IRIS.csv\")  \n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83b7987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "***labels value : ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pr('labels value : ' +str(iris['species'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b098187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x156bcea6310>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 537.225x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.FacetGrid(iris,hue='species',height=6).map(plt.scatter,'sepal_length','sepal_width').add_legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41c626",
   "metadata": {},
   "source": [
    " We will express the label values as <mark> one-hot encoding </mark> variables or so-called dummies variables. equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acd241ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "***One hode encoding representation***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Iris-setosa  Iris-versicolor  Iris-virginica\n",
       "0            1                0               0\n",
       "1            1                0               0\n",
       "2            1                0               0\n",
       "3            1                0               0\n",
       "4            1                0               0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = iris.drop('species', axis=1)\n",
    "y_train = pd.get_dummies(iris['species'])\n",
    "pr('One hode encoding representation')\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c27368ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "***shape X :(150, 4)***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "***shape X :(150, 3)***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "pr('shape X :'+ str(x_train.shape))\n",
    "pr('shape X :'+ str(y_train.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a2ad5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_train,y_train, test_size=0.33, random_state=42) #separats into test and train samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d960beb",
   "metadata": {},
   "source": [
    "<h2  id=\"deff_softmax\" >Softmaxt definition and  how it works?</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b0b59",
   "metadata": {},
   "source": [
    " The softmax function $\\sigma: \\; \\Re^k \\; \\rightarrow \\; \\Re^k $  could be defined by formula :\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2a90d",
   "metadata": {},
   "source": [
    "$$ (1) \\;\\; \\sigma_{softmax}({z^i})_{ij} =\\frac {e^{z_{ij}} }{ \\sum_i^k e^{z_{ik}} } $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb6840",
   "metadata": {},
   "source": [
    "\n",
    "$\\; \\; z_{ij} = \\sum_p x_{ip} w_{jp} + b_j $ \n",
    " <br><br>\n",
    "$\\; \\; \\vec{z_i} = z^i= [z_{i1}, z_{i2}, ...z_{ik} ] $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe1eb5",
   "metadata": {},
   "source": [
    " $$\\sigma_{softmax (W,b,X^{i})_{ij}}=\\frac{e^{ \\sum_p x_{ip} w_{pj} + b_j}}{\\sum_j^k e^{ \\sum_p x_{ip} w_{pj} + b_j}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991d0b2f",
   "metadata": {},
   "source": [
    "###### Index convetions. When we note for example $Q_{lh}$ or $q_{ih}$ we will mean the element of matrix $Q_{M\\times N}$ where $l$ refers to row $h$ to column. When we write $Q^{i}$  or  $q_{i}$ we  will assume that as vector row  $\\vec Q^{i}= [q_{i1},q_{i2},...q_{in}]\\in Q_{m\\times n}$. In eq(1) $z^i=[z_{i1}, z_{i2}, ...z_{in}] $\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d7c149",
   "metadata": {},
   "source": [
    " The softmax function  takes as an input a vector $z^i$ with $K$ number of component $z_{i1}, z_{i2}, ...,z_{in}$  and normalized it into  a  probability distribution $p^{i}$ consisting of also  $K$ number of probabilities  $p_{i1},...,{ik}$ proportional to exponentials of input values $z^{i}$. That is, prior to applying softmax function some vector components of $z^{i}$ could be negative or greater than 1 and  might not sum up to 1. Furthermore more the larger input components correspond to larger probabilities and $\\sum_j p_{ij}=1$. The $w_{jp}$ are the wights or estimators  $w_{pj}\\in W^{K\\times N}$ In matrix  $W^{K\\times N}$ $K$ coresponds to number of class labels and  $N$ coresponds to number of atrribute(feature) of training data, and $ b = [b_1, ...b_k]$ is bais term with component coresponds to class labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db066805",
   "metadata": {},
   "source": [
    "This property of softmax function that it outputs a probability distribution makes it suitable for probabilistic interpretation in classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861002e",
   "metadata": {},
   "source": [
    "According to our dataset, we can write the folowing expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89cdf93",
   "metadata": {},
   "source": [
    "$W= \\begin{bmatrix} \n",
    "      weight^1\\rightarrow class \\; 1(Iris-setosa)\n",
    "      \\\\ weight^2\\rightarrow class\\;  2(Iris-versicolor) \\;  \n",
    "      \\\\ weight^{3}\\rightarrow class \\;3(Iris-virginica) \\;\n",
    "      \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "    \\vec W^1 \\\\  \\vec W^2\\  \\\\ \\vec W^3  \\end{bmatrix} =\n",
    "    \\begin{bmatrix} \n",
    "    w_{11} & w_{12} & w_{13} & w_{14}\n",
    "    \\\\ w_{21} & w_{22} & w_{23}  & w_{24}\n",
    "    \\\\ w_{31} & w_{32} & w_{33} & w_{34}\n",
    "    \\end{bmatrix}  $ &nbsp;&nbsp;&nbsp;&nbsp;  $ B= \\begin{bmatrix}  b_1 \\\\  b_2\\  \\\\  b_3  \\end{bmatrix}\\;\\;\\;$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f94bb",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vector $\\vec W^i=[w_{i1},..w_{in}]$ is estimator vector for taget class (label) $i$ , $n$ coresponds to the feature (pridictor) of $X$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0f744",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;in matrix form the $Z$ is expressed as : $Z = XW^T$ \n",
    " <br> <br>\n",
    " \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;  $Z = \n",
    "    \\begin{bmatrix} x_{11} &  x_{12} & x_{13}  & x_{14} \\\\  x_{21} &  x_{22} & x_{23}  & x_{24}\\\\ ... & ... & ... & ...\\\\ x_{m1} &  x_{m2} & x_{m3}  & x_{m4} \\end{bmatrix} \\times \\begin{bmatrix}\n",
    "    w_{11} & w_{21} & w_{31} \n",
    "    \\\\ w_{12} & w_{22} & w_{32} \n",
    "    \\\\  w_{13} & w_{23} & w_{33}\n",
    "     \\\\  w_{14} & w_{24} & w_{34}\n",
    "    \\end{bmatrix} + \\begin{bmatrix}  b_1 \\\\  b_2\\  \\\\  b_3  \\end{bmatrix}  =   \\begin{bmatrix} z_{11} &  z_{12} & z_{13} \\\\  z_{21} &  z_{32} & z_{33} \\\\ ... & ... & ...  \\\\ z_{m1} &  z_{m2} & z_{m3}  \\end{bmatrix} $ <br>   \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6cb056",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp; The softmax function computes the probability that a training example $X^{(i)}$  belongs to class $y^{j}$ given \n",
    "    the weight  matrix $W$ and bias $\\vec b$ . <br> \n",
    "    So we compute the probability : <br>\n",
    "\n",
    "$$p_{ij}=P(y_{j} \\;| \\;z^i) = \\sigma_{softmax (z)_{ij}}=\\frac{e^{z_{ij}}}{\\sum_p^K e^{z_{ip}}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81924d8e",
   "metadata": {},
   "source": [
    "\n",
    "&nbsp;&nbsp; for all $P_{ij}$ and given target components  $Y = [y^1 -Iris-setosa,y^2 -Iris-versicolor,y^3- Iris-virginica]$ we can write :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d551e2a",
   "metadata": {},
   "source": [
    " $$ P = \\begin{bmatrix} p(y_{1} |z^{1})_{11} &  p(y_{2} |z_{2} )_{12}  &  p(y^{3}  |z^{3} )_{13} \n",
    "        \\\\ ... &  ...  & ...\n",
    "        \\\\ P(y_{1} |z^{m}  )_{m1} &  P(y_{m}  |z^{m}  )_{m2}  & P(y_{3}  |z^{m} )_{m3}\n",
    "        \\end{bmatrix} = \n",
    "        \\begin{bmatrix} \\frac{e^{z_{11}}}{\\sum_{1j}e^{z_{1j}}} & \\frac{e^{z_{12}}}{\\sum_{1j}e^{z_{1j}}} & \\frac{e^{z_{13}}}{\\sum_{1j}e^{z_{1j}}} \n",
    "        \\\\   \\\\ ... & ... & ...  \\\\\n",
    "        \\\\    \\frac{e^{k_{n1}}}{\\sum_{j}^3e^{z_{nj}}} & \\frac{e^{z_{n2}}}{\\sum_{j}^3e^{z_{nj}}} & \\frac{e^{z_{n3}}}{\\sum_{j}^3e^{z_{nj}}}\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b5ae0",
   "metadata": {},
   "source": [
    "for example, the probability of record $x^1$ to belongs to target label  $y_2$ (Iris-versicolor) is calculated as:<br> <br> \n",
    "\n",
    "$$p_{12} =P(y=2 |x^{1}) = \\frac{ e^{ ^{z_{12}} } }{ \\sum_p^3 e^{z_{1p}}}=\\frac{ e^{ (^{\\sum_v^3 x_{1v}.w_{vj} + b_v }} )}{ \\sum_k^3 e^{ ^ ({\\sum_v^3 x_{1v}.w_{vk}} + b_k})}$$\n",
    "always the $p_{ij} \\in [0,1]$   and $\\sum_j p_{ij}= 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f348766",
   "metadata": {},
   "source": [
    " &nbsp; Let's see how the softmax function can be applied concretely in our training dataset.\n",
    "             First, let to define a weight matrix $W$ and bias $\\vec b$ <br> <br>\n",
    "\n",
    "\n",
    "   &nbsp;&nbsp;    $W =\\begin{bmatrix} \n",
    "    w_{11} & w_{12} & w_{13} & w_{14}\n",
    "    \\\\ w_{21} & w_{22} & w_{23}  & w_{24}\n",
    "    \\\\ w_{31} & w_{32} & w_{33} & w_{34}\n",
    "    \\end{bmatrix} \n",
    "    =  \\begin{bmatrix} 1.38618464 &  1.9151765 & -0.28863154 & 0.40849489\n",
    "              \\\\1.31642223 & 0.76753677 &  1.1482473 & 0.74274245\n",
    "              \\\\0.29739313 & 0.31728673 & 2.14038423 &  1.84876265\\end{bmatrix}$ <br> <br>\n",
    "     &nbsp;&nbsp; $B = \\begin{bmatrix}  1.18749764 \\\\ 1.16215506 \\\\0.6503473   \\end{bmatrix}  $\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38e125",
   "metadata": {},
   "source": [
    " I've prepared weight vector $W$ and bias $B$ in advance, how? We will see later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2181be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[ 1.38618464,  1.9151765 , -0.28863154,  0.40849489],\n",
    "        [ 1.31642223,  0.76753677,  1.1482473 ,  0.74274245],\n",
    "        [ 0.29739313,  0.31728673,  2.14038423,  1.84876265]]) # define a weight matrix\n",
    "\n",
    "b = np.array([1.18749764, 1.16215506, 0.6503473 ])  #bias vector (intercept) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a83258",
   "metadata": {},
   "source": [
    " The implementation of softmax function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "928bcf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, weight, b):\n",
    "    '''\n",
    "    perform softmax function\n",
    "    \n",
    "    Parameters :\n",
    "    X : ndarray\n",
    "       train data\n",
    "    weight : ndarray\n",
    "       weght matrix\n",
    "    b : ndarray \n",
    "        bias vector        \n",
    "\n",
    "    Returns \n",
    "       ndarray \n",
    "    '''\n",
    "   \n",
    "    #dot product between X_data matrix  and tranposed Weight_ matrix added to Bias  gives matrix each z_ij\n",
    "    Z = X.dot(weight.T)\n",
    "\n",
    "    #return matrix cosist of exponentials Z input net\n",
    "    exp_z = np.exp(Z)\n",
    "\n",
    "    #array contains sum  of every row  (e^z_{ik})\n",
    "    sums=np.sum(exp_z, axis=1) \n",
    "    \n",
    "    #return softmax(Z)_{ij}\n",
    "    return (exp_z.T/sums).T \n",
    "\n",
    "\n",
    "def accuracy(Y, P):\n",
    "    '''\n",
    "    evualate accuracy of dummies variable\n",
    "    \n",
    "    Parameters :\n",
    "    Y_target : ndarray\n",
    "      actual real values \n",
    "    P : ndarray\n",
    "     predicted values (probability)\n",
    "    Return\n",
    "      float\n",
    "    '''\n",
    "        \n",
    "    C =  np.argmax(Y, axis=1)==np.argmax(P, axis=1)\n",
    "    D = np.where(C==True)\n",
    "    return len(D[0])/len(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b551e61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "***original data label $ Y_{M\\times N} :$***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Iris-setosa  Iris-versicolor  Iris-virginica\n",
       "0            0                1               0\n",
       "1            0                0               1\n",
       "2            0                1               0\n",
       "3            1                0               0\n",
       "4            0                0               1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr( \"original data label \" +r'$ Y_{{M\\times N}} :$'.format(X_test.shape))\n",
    "actual_label = pd.DataFrame(y_train,columns = ['Iris-setosa', 'Iris-versicolor','Iris-virginica'])\n",
    "actual_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51ece1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "***predicted data label $ P_{M\\times N} :$***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050053</td>\n",
       "      <td>0.777979</td>\n",
       "      <td>0.171968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.555302</td>\n",
       "      <td>0.443584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.030317</td>\n",
       "      <td>0.696057</td>\n",
       "      <td>0.273627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.907907</td>\n",
       "      <td>0.091569</td>\n",
       "      <td>0.000524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.561198</td>\n",
       "      <td>0.437995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Iris-setosa  Iris-versicolor  Iris-virginica\n",
       "0     0.050053         0.777979        0.171968\n",
       "1     0.001113         0.555302        0.443584\n",
       "2     0.030317         0.696057        0.273627\n",
       "3     0.907907         0.091569        0.000524\n",
       "4     0.000807         0.561198        0.437995"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr(\"predicted data label \" +r'$ P_{{M\\times N}} :$'.format(X_test.shape))\n",
    "predict = softmax(X_train,W,b)\n",
    "predict = pd.DataFrame(predict,columns = ['Iris-setosa', 'Iris-versicolor','Iris-virginica'])\n",
    "predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d32ff2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "***accuracy : 0.75***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a =accuracy(np.array(y_train),np.array(predict))\n",
    "pr('accuracy : '+str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf47578",
   "metadata": {},
   "source": [
    "From $P_{{M\\times N}}$ output let to consider the row with index  $1$ : <br> <br>\n",
    "            $P'_{1,1} = 0.004 \\rightarrow  $ has  $0$% chance that record $X^{1}$ belongs to class 1 'iris-setosa' <br> \n",
    "            $P'_{1,2} = 0.167 \\rightarrow  $ has $17$% chance that record  $X^{1}$ belongs to class 2 'Iris-versicolor' <br> \n",
    "            $P'_{1,3} = 0.83 \\rightarrow  $ has $83$% chance  belongs to class 3 'Iris-virginica'<br> \n",
    "            <br>\n",
    "      From above result we can conclude : <br>\n",
    "      We cannot be too sure which is the class label that record  $X_{1}$ belongs,but its third column has $80$% chance which is the biggest one therefore, we would assume that record belongs to class 'Iris-virginica'in fact, that is coorect assumtion comparing with actual data $Y$. By applying this evaluation process for all results we can validate that the weight matrix $W_{M \\times N}$ and $b$ have given the $95$% accuracy.&nbsp;&nbsp; &nbsp;&nbsp; <br> <br> How have I  found the weight matrix $W$ and bias $B$ ? \n",
    "Just I've used the <mark>LogisticRegression</mark> from <mark>scikit-learn</mark> and took the coefficients, let us try to find out the way   of finding the weight $W$ and bias $b$ and is it posible to imporove estimators of $W$ and bias $b$.\n",
    "\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3773343",
   "metadata": {},
   "source": [
    "<h2>Optimizaton of  Softmax Loss with Gradient Descent (Deep math) </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77fafbc",
   "metadata": {},
   "source": [
    "For objective loss function we will use <mark>Cross-Entropy</mark> wich is used binary logistic reg.Sofmax Loss  is defined as :\n",
    "\n",
    "   $$ (2)\\;\\; \\mathcal{L}(Y,Z)=-\\sum_i^m\\sum_j^k y_{ij} \\log (p(Z)_{ij})$$ <br> <br>  \n",
    "\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "    Where $m$ is a count of records , $k$ is  count of classes $y_{ij}$ is label values , $p_{ij}=\\phi_{softmax}(Z)_{ij}$ are  Y' predicited class  values and $Z$ is net input which is function of $W$ is weight matrix and $b$ bias and $X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b5d927",
   "metadata": {},
   "source": [
    "Our goal is to minimize the eq.(2) in order to find the best estimators $w_{ij}\\in W$ and $b$ given the iris data $X$ and a label data $Y$. <br>\n",
    "We are going to use Gradient descent for optimization process. \n",
    "Note that, the eq.(2) is the function of all weights $w_{ij}$, bias $b_j$ all training data X and label data Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4294dd3b",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;  Gradient descent is defined as  : \n",
    "<br>    \n",
    "$$ \\;  \\; \\;  \\; \\; \\; \\;\\begin{matrix} w_{ij} = w_{ij} - \\lambda \\nabla w_{ij}L(W,b,X,Y) \\\\  \\\\ b_{j} = b_j - \\lambda\\nabla b_{j}L(W,b,X,Y)\n",
    "       \\end{matrix} $$ \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "where  $\\lambda$ is learning rate or step size \n",
    "   \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780af0dd",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp; Plug eq.(2) in gradient descent we achieve the general formula : <br> <br>\n",
    "     $$(3) \\;  \\; \\;  \\; \\; \\; \n",
    "   \\begin{matrix} \\nabla w_{ij}L(Y,P)   &=  \\nabla w_{ij}L(Y,P) -\\frac{\\partial}{\\partial w_{ij}}\\Big(\\sum_k^m\\sum_n^n y_{mn} \\log {p_{mn}}\\Big) \\\\  \\\\\n",
    "     \\nabla b_{j}L(Y,P) & = \\nabla b_{j}L(Y,P)  - \\frac{\\partial}{\\partial b_{j}}\\Big(\\sum_k^m\\sum_n^n y_{mn} \\log{p_{mn}}\\Big)\n",
    "       \\end{matrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf803cf",
   "metadata": {},
   "source": [
    "Before we take up  with  $\\nabla w_{ij}L(W,b)$. We gonna introduce some math technics which will make our work easier.\n",
    "   <br>\n",
    "   For simplicity in the summation process of indices, we will introduce a <a href='http://physics.csusb.edu/~prenteln/notes/vc_notes.pdf'>Kronecker symbol</a> .<br>   <br>\n",
    "</font>\n",
    "<font size=\"3\" color='#4a3e20' >  \n",
    "       $$\\delta_{ij} =    \n",
    "         \\begin{equation}\n",
    "   \\begin{Bmatrix} \n",
    "   1 & if \\; i=j  \\\\\n",
    "   0 & if \\; i\\ne j  \n",
    "    \\end{Bmatrix} \n",
    "\\end{equation}$$\n",
    "  <br><br>\n",
    "          $$ \\delta_{ij} = \\begin{bmatrix} 1 & 0 & 0  \\\\ 0 & 1 & 0 \\\\  0 & 0 & 1 \\end{bmatrix}$$\n",
    "   </font>\n",
    "<font size=\"3\" color='#270336' face = \"Times New Roma\">  \n",
    "   In many places in the coming sum operations over indexes we will miss the $\\sum$ symbol, just it will be avoided(hidden) according to the .<a href='https://en.wikipedia.org/wiki/Einstein_notation'>Einstein summation convention</a> .<br>\n",
    "       For example, the equation. \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df1cac",
   "metadata": {},
   "source": [
    "​\n",
    "<h7>\n",
    "    <font size=\"2\" color='#270336'>  \n",
    "   $$z_{ij} = \\sum_p^3 x_{ip} w_{jp} + b_j $$ \n",
    "</font>\n",
    "<font size=\"3\" color='#270336' face = \"Times New Roma\">  \n",
    "     by applying the  Enstein convetion we could rewrite it as : <br>\n",
    "</font>\n",
    "</h7>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff8f691",
   "metadata": {},
   "source": [
    " $$z_{ij} = x_{ip} w_{jp} + b_j$$  \n",
    "The sign $\\sum_p^3$ is miss.The sumation over p  is implied(by default) because p is repeated twice.Every time when there are repeatable indices that is the indicator for exist of $\\sum$  which is just missing(The sum sign  is not written).\n",
    "\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7be5ef",
   "metadata": {},
   "source": [
    "  &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;In order to minimize the entropy of data, we must to take up with minimizatin of Cross entropy loss respect to $w_{ij}$.The cross-etropy Loss is a function of all feature vectors $X_{M\\times N}$ ,all labels $X_{M\\times K}$ , weight $W_{K\\times N}$ and bias $B_K$ <br> $L = L(X,T,W,B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ead72",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$\\frac{\\partial L}{\\partial w_{ij}}=-\\frac{\\partial}{\\partial w_{ij}}\\Big(\\sum_k\\sum_n y_{mn} \\log {p_{mn}}\\Big)$\n",
    " $=\\sum_k\\sum_n y_{mn}\\frac{\\partial \\log {p_{mn}}}{\\partial w_{ij}}$ \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3623b1c",
   "metadata": {},
   "source": [
    " \n",
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$=-\\sum_m\\sum_n\\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial w_{ij}} $ \n",
    "            $=\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial z_{vp}}\\frac{\\partial  z_{vp}}{\\partial w_{ij}} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aacd7a",
   "metadata": {},
   "source": [
    "<h6> from  $z_{vp} = f(w_{pi}) \\Rightarrow \\frac{\\partial  z_{vp}}{\\partial w_{ij}} = 0\\;$ if $\\; p\\ne i$ then  we can  write $ \\frac{\\partial  z_{vp}}{\\partial w_{ij}}=\\delta_{pi}\\frac{\\partial  z_{vp}}{\\partial w_{ij}}$ \n",
    "      and  $p_{mn} = f(z_{mv}) \\Rightarrow \\frac{\\partial  z_{vp}}{\\partial w_{ij}} = 0$ if $ m\\ne v $ then   $\\frac{\\partial p_{mn}}{\\partial z_{vp}}=\\delta_{mv}\\frac{\\partial  p_{mn}}{\\partial z_{vp}}  $ \n",
    "   plug in it  we will achieve </h6>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f61a04",
   "metadata": {},
   "source": [
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$ =-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\delta_{mv}\\frac{\\partial  p_{mn}}{\\partial  z_{vp}} \\delta_{pi}\\frac{\\partial  z_{vp}}{\\partial w_{ij}} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6e783",
   "metadata": {},
   "source": [
    "<h6>using common Kronicker $\\delta$ proprties </h6> <br> \n",
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$=-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\delta_{mm}\\delta_{ii}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}} $ $=-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}} $ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be08cb",
   "metadata": {},
   "source": [
    "We've successfully reduced the count of sum operations, using Einstein's convention and Kronecker symbol and\n",
    "    achieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b142c7",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$\\frac{\\partial L}{\\partial w_{ij}}=-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2746dd",
   "metadata": {},
   "source": [
    "  Let to focus on terms $\\frac{\\partial  p_{mn}}{\\partial z_{mi}}$ and $\\frac{\\partial  z_{mi}}{\\partial w_{ij}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ebdcb",
   "metadata": {},
   "source": [
    "   $\\frac{\\partial  p_{mn}} {\\partial z_{mi}}=\\frac{\\partial\\frac { e^{z_{mn}} }{ \\sum_ke^{z_{mk}}} }{\\partial z_{mi}}$\n",
    "     $=\\frac{1}{(\\sum_ke^{z_{mk}})^2}\\times \\Big(\\frac{\\partial e^{z_{mn}} }{\\partial z_{mi}}\\times(\\sum_ke^{z_{mk}}) - e^{z_{mn}}\\times\\frac{\\partial (\\sum_ke^{z_{mk}})}{\\partial z_{mi}}  \\Big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef848e",
   "metadata": {},
   "source": [
    "  $=\\frac{e^{z_{mn}}\\times\\frac{\\partial z_{mn}}{\\partial z_{mi}}}{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mk}} \\frac{ \\partial z_{mk}}{\\partial z_{mi}}}  {\\sum_ke^{z_{mk}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ccf77",
   "metadata": {},
   "source": [
    "<h6> from  $\\frac{\\partial z_{mk}}{\\partial z_{mi}}=0$ if $k\\ne i\\;\\frac{\\partial z_{mk}}{\\partial z_{mi}}=1\\;ifk = i\\;\\Rightarrow \\frac{\\partial z_{mk}}{\\partial z_{mi}}=\\delta_{ki} $ plug in </h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9782843",
   "metadata": {},
   "source": [
    " $=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mk}}\\delta_{ki}}  {\\sum_ke^{z_{mk}}}$ \n",
    "      $=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_k e^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mk}}\\delta_{ki}}  {\\sum_ke^{z_{mk}}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb4f45",
   "metadata": {},
   "source": [
    "  $=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mi}}\\delta_{ii}}  {\\sum_ke^{z_{mk}}}$ \n",
    "          $=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times \\frac{ e^{z_{mi}}}{\\sum_ke^{z_{mk}}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a64b7d",
   "metadata": {},
   "source": [
    "<h6>from eq.(1)  $\\Rightarrow$ $\\frac{ e^{z_{mn}} }{ \\sum_k e^{z_{mk} } }=p_{mn}$ and $\\frac{ e^{z_{mi}} }{ \\sum_k e^{z_{mk} } }=p_{mi}$  when we apply it, we will achieve </h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d119fafe",
   "metadata": {},
   "source": [
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "       $=p_{mn}\\times \\delta_{ni} - p_{mn}\\times p_{mn}p_{mi}$ <br>\n",
    "        &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "          $=p_{mn}(\\delta_{ni} -  p_{mi})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96737513",
   "metadata": {},
   "source": [
    " For term  $\\frac{\\partial  p_{mn}} {\\partial z_{mi}}$ we achieve : <br>\n",
    "  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $$4) \\; \\; \\; \\; \\frac{\\partial  p_{mn}} {\\partial z_{mi}}=p_{mn}(\\delta_{ni} -  p_{mi})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b970ba",
   "metadata": {},
   "source": [
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $\\frac{\\partial  z_{mi}}{\\partial w_{ij}} =\\frac{\\partial ( \\sum_k  x_{mk}w_{ki})}{\\partial w_{ij}}= \\frac{ \\sum_k z_{mi} x_{mk}\\partial w_{ki}}{\\partial w_{ij}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b960256",
   "metadata": {},
   "source": [
    "<h6>  $\\frac{\\partial w_{ki}}{\\partial w_{ij}} = \\delta_{kj}$ only a direct verification can  proof it </h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7afb42",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $=  \\sum_k  x_{mk} \\delta_{kj} =\\sum_k  x_{mj}\\delta_{jj}=x_{mj}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17975516",
   "metadata": {},
   "source": [
    "<h6> and for $\\frac{\\partial  z_{mi}}{\\partial w_{ij}}$ we achive : </h6>\n",
    "      $$(5) \\;\\;\\;\\;\\;\\frac{\\partial  z_{mi}}{\\partial w_{ij}} = x_{mj} $$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6a8755",
   "metadata": {},
   "source": [
    "<h6>  &nbsp;&nbsp;\n",
    "     Applying  eqs.(4) (5) in : $\\frac{\\partial L}{\\partial w_{ij}}=-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}}$   we have  : </h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35cb46",
   "metadata": {},
   "source": [
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$\\frac{\\partial L}{\\partial w_{ij}}=-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}p_{mn}(\\delta_{ni} -  p_{mi})x_{mj}=--\\sum_m\\sum_n y_{mn}(\\delta_{ni} -  p_{mi})x_{mj} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9f3510",
   "metadata": {},
   "source": [
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $ =-\\sum_m\\sum_n y_{mn}\\delta_{ni} + \\sum_m\\sum_n y_{mn} p_{mi}x_{mj} $ \n",
    "<h6> we can  replace an index $n$ with $i$ (!There is no $\\sum_i$)  </h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c1b44",
   "metadata": {},
   "source": [
    "  $ =-\\sum_m\\sum_n y_{mi}\\delta_{ii}x_{mj} + \\sum_m\\sum_n y_{mn} p_{mi}x_{mj} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8394a6",
   "metadata": {},
   "source": [
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "   $ =-\\sum_m y_{mi}x_{mj} + \\sum_m\\sum_n y_{mn} p_{mi}x_{mj} $ <br>\n",
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "      $\\sum_n y_{in}=1$ <h6> (the sum of all probability) applying it </h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28660d21",
   "metadata": {},
   "source": [
    " $ =-\\sum_m y_{mi}x_{mj} + \\sum_m 1. p_{mi}x_{mj} = \\sum_m  p_{mi}x_{mj}-\\sum_m y_{mi}x_{mj}$ <br> <br>\n",
    "  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "  $ = \\sum_k^m( p_{mi}-y_{mi})x_{mj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f832a1cb",
   "metadata": {},
   "source": [
    "We've achieved the most important result .The optimization  of Cross-entropy respect to $w_{ij}$  :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7bfb51",
   "metadata": {},
   "source": [
    " $$6) \\;\\;\\;\\;\\nabla w_{ij}L(W,b)= \\sum_m( p_{mi}-y_{mi})x_{mj}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7628f1b9",
   "metadata": {},
   "source": [
    " If we apply  the same   step for $\\nabla b_{i}L(W,b)$ (It is an easier one)  we will achieve the minimization formula for bias , which looks like this<br><br>\n",
    "      $$7)\\;\\;\\;\\;\\nabla b_{i}L(W,b)= \\sum_m( p_{mi}-y_{mi})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c72ab6",
   "metadata": {},
   "source": [
    "Although the eq.(6) seems so simple and elegant it is written in a tensor form, not in matrix one. Therefore its implementation becomes more difficult, especially when we want to use our lovely library NumPy.But we can write the equation in matrix form seeming like that :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90618bb",
   "metadata": {},
   "source": [
    " $\\nabla_W L = \\begin{bmatrix}\n",
    "               \\nabla_{w_{11}} L  & \\nabla_{w_{12}}L &... &\\nabla_{ w_{1j}}L \\\\\n",
    "               \\nabla_{ w_{21}}L & \\nabla_{ w_{22}}L  &... &\\nabla_{ w_{2j}}L \\\\\n",
    "                ...   &  ...  & ... & ...  \\\\\n",
    "                \\nabla_{ w_{i1}}L  & \\nabla_{ w_{i2}}L  & ...& \\nabla_{ w_{ij}}L \\end{bmatrix} $\n",
    "$  =\\begin{bmatrix}\n",
    "p_{11} -y_{11}  &  p_{21}-y_{21}  & ... &  p_{m1}-y_{m1}\\\\\n",
    "p_{12} -y_{12}  &  p_{22}-y_{22}  & ... &  p_{m2}-y_{m2}\\\\\n",
    "\\;\\;...\\;\\;\\;   &  \\;\\;...\\;\\;\\;  &\\;\\;...\\;\\;\\; &\\;\\;...\\;\\;\\; \\\\\n",
    "p_{1i} -y_{1i}  &  p_{2i}-y_{2i}  & ... &  p_{mi}-y_{mi}\n",
    " \\end{bmatrix}$\n",
    "$\\begin{bmatrix} x_{11} &  x_{12} &...&  x_{1j} \\\\  x_{21} &  x_{22}  &...&  x_{2j}  \\\\ ... & ... & ... &... \\\\ x_{m1} &  x_{m2} &...&  x_{mj}\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd4294",
   "metadata": {},
   "source": [
    "$$8)\\;\\;\\;\\;\\nabla_W L = (P-Y)^T.X$$ <br>\n",
    "$$\\;\\;\\;\\;\\nabla_{b_i} L = \\sum_m (P-Y)_{mi}\\;\\;\\; or $$ or <br> <br>\n",
    " $$9)\\;\\;\\;\\;\\nabla_{b} L =\\Big[\\sum_m (P-Y)_{m1}, \\sum_m (P-Y)_{m2},..., \\sum_m (P-Y)_{mi}\\Big]$$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428b967a",
   "metadata": {},
   "source": [
    " We've  written eq(6) and(7) in matrix form, and  result is surprisingly simple and  so easy for implementation.If we plug the eq(8) and (9) in gradient descent eqaution <br> \n",
    " $$ \\;  \\; \\;  \\; \\; \\; \\;\\begin{matrix} w_{ij} = w_{ij} - \\lambda \\nabla w_{ij}L(W,b) \\\\  \\\\ b_{j} = b_j - \\lambda\\nabla b_{j}L(W,b)\n",
    "       \\end{matrix} $$ \n",
    "\n",
    " We've achieve our minimization alogithm for Cross-entopy Loss for find the best esitmitators $W_{ij}$.\n",
    "    The minimization alogorithm using gradient descent is deifined as :  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0afca0",
   "metadata": {},
   "source": [
    " $$(10) \\;  \\; \\;  \\; \\; \\; \\;  W= W -\\lambda(P-Y)^T.X  $$\n",
    "  <br>\n",
    "    $$(11) \\;  \\; \\;  \\; \\; \\; \\;  b_i = b_i -\\lambda \\sum_m(P-Y)_{mi}   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772a414",
   "metadata": {},
   "source": [
    "where W is a weight matrix , $\\lambda$ is an leraning rate or step size, P is the the prediction values pruduced from sofmax   $Y$ is the target values $X$ is the training data (features vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9076a6d2",
   "metadata": {},
   "source": [
    "<h2> Implementation of Softmax using numpy </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2201fe6",
   "metadata": {},
   "source": [
    "   &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;The implentation  using <mark color='blue' size='5'>numpy</mark>\n",
    "     could be defined as : <br> <br>:\n",
    "\n",
    "  $$ W = W - gamma*(( softmax(W,b,X,Y) -Y )^T).dot(X)$$ <br>\n",
    "      $$ b_i = b_i - gamma*sum_m((softmax(W,b,X,Y)_i -Y_i)) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e207ef",
   "metadata": {},
   "source": [
    "where the arguments are considered: the W matrix our estimator coeficients , $gamma=$ $\\lambda* (1/m)$  step size,\n",
    "     $\\lambda$ learning rate $Y$ is the target values $X$ is the training data (features vectors).\n",
    " The implementation of gradient descent <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "064b8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, W, b, step_size):\n",
    "    ''''\n",
    "    one iteration(Epoch) perform  gradient descent\n",
    "    \n",
    "     Parameters :\n",
    "    X : ndarray\n",
    "       train data\n",
    "    y : ndarray\n",
    "       target data\n",
    "    W : ndarray\n",
    "     weight matrix\n",
    "    b : ndarray\n",
    "      bias \n",
    "    step_size : float\n",
    "       gradient descent setting\n",
    "    \n",
    "    '''\n",
    "    P_y = softmax(X,W,b)-y\n",
    "    W = W - step_size*(P_y.T).dot(X)\n",
    "    b = b - step_size*np.sum(P_y, axis=0)\n",
    "    return W,b\n",
    "    \n",
    "\n",
    "def train(X , y, max_iter=100,learning_rate=0.1,innitial_value =1, debug_W=None):\n",
    "    '''\n",
    "    Train by softmax regression \n",
    "    \n",
    "    Parameters :\n",
    "    X : ndarray\n",
    "       train data\n",
    "    y : ndarray\n",
    "       target data\n",
    "    max_iter : int \n",
    "        number of epoch (iterations)\n",
    "        \n",
    "    debug_W : tuple \n",
    "       index of weight parameter for debugging\n",
    "    Returns \n",
    "       W, b : ndarray\n",
    "         weight and bias\n",
    "       in debug mode\n",
    "       W, b, k : ndarray\n",
    "          weight and bias and debugind parameter \n",
    "         \n",
    "       \n",
    "    '''\n",
    "    \n",
    "    if type(X) != np.ndarray or type(y) != np.ndarray: \n",
    "        raise ValueError('X and y must be ndarray')\n",
    "        \n",
    "    #init weight and bias\n",
    "    b = np.full((y.shape[1],),innitial_value)\n",
    "    W = np.full((y.shape[1], X.shape[1]), innitial_value)\n",
    "    \n",
    "    m = X.shape[0] \n",
    "    step_size = (1/m)*learning_rate\n",
    "    \n",
    "    if debug_W is not None: \n",
    "        debug_mode=True \n",
    "        debug = W[(debug_W)]\n",
    "    else :\n",
    "        debug_mode=False\n",
    "        \n",
    "    for i in range(max_iter):\n",
    "        W,b = gradient_descent(X , y, W, b,step_size)\n",
    "        if debug_mode: debug = np.append(debug, W[debug_W])   \n",
    "    \n",
    "    \n",
    "    if debug_mode: return W,b,debug\n",
    "        \n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f5a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "291b307c",
   "metadata": {},
   "source": [
    " Let to test our implementation and to train the iris data  <br> \n",
    " Trainig data<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd81ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "W,b = train(np.array(X_train), np.array(y_train),max_iter=100)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43634123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight vector :[[ 1.35411091  1.84190926 -0.19105989  0.45297526]\n",
      " [ 1.27780858  0.77747064  1.17029213  0.79586893]\n",
      " [ 0.36808051  0.3806201   2.02076776  1.75115581]]\n",
      "\n",
      "bias :[1.17282574 1.13488628 0.69228799]\n"
     ]
    }
   ],
   "source": [
    "print('weight vector :'+ str(W))\n",
    "print('')\n",
    "print('bias :'+ str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d2f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2f0b3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "***accuracy:  0.7%***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict = softmax(np.array(X_test), W,b)\n",
    "#print(v)\n",
    "pr('accuracy:  ' +str(accuracy(np.array(y_test), predict))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d392c32",
   "metadata": {},
   "source": [
    " We've  achieved  96% acuracy , learning rata = 0,1 max iteration 220<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d91e02",
   "metadata": {},
   "source": [
    "Testing  on  data X_test and  y_test <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649587d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
