import numpy as np
import pandas as pd
import collections


def entropy(y):
    """ Entropy measures the degree of uncertainty,
     impurity or disorder of a random variable

    :param target: (list) target values
    :return: (float)  entropy of the array
    """

    counter = collections.Counter(y)
    p = np.array(list(counter.values())) / len(y)
    return -np.sum(p * np.log2(p))


def information_gain(y, mask, func=entropy):
    """Information gain is used for determining the best
    features/attributes that render maximum information about a class.

    :param y: (list) target values
    :param mask:(list of booleans) used for splitting the data
    :param func: (function) function for measurements of impurity/uncertainty
    :return: (float) Information Gain
    """
    if isinstance(y, pd.Series):
        y_right = y[mask]
        y_left = y[-mask]
        size = len(y)
        w_right, w_left = len(y_right) / size, len(y_left) / size
        return entropy(y) - (w_right * entropy(y_right) + w_left * w_left)

    else:
        raise TypeError("y must be a Pandas Series")


def best_split_value(series, y, split_type):
    values = series.unique()
    inf_gain_values = []
    feature_values = []

    for val in values:
        if split_type == '<=':
            mask = series < val
        else:
            mask = series >= val

        i_g = information_gain(y, mask, func=entropy)
        inf_gain_values.append(i_g)
        feature_values.append(val)

    max_inf_gain = np.max(inf_gain_values)
    index = inf_gain_values.index(max_inf_gain)
    best_feature_value = feature_values[index]

    return best_feature_value, max_inf_gain


def split_data(x, y, question):
    pass


class Node:
    split_type = '<'

    def __init__(self):
        self.left_interior = None
        self.right_interior = None
        self.data = None
        self.question = None

    @classmethod
    def generate_decision_node(cls, X, y):
        """

        :return: Node
        """

        node = Node()

        feature, value = cls.best_split(x, y)
        question = feature + Node.split_type + value
        node.question = question
        left_data, right_data = split_data(data, y, question)

        node.left_interior = cls.generate_decision_node(left_data, y_left)
        node.right_interior = cls.generate_decision_node(right_data, right_left)

        return node

        root = self._generate_node(X, y)

    @staticmethod
    def best_split(x, y):
        k = x.apply(best_split_value, y=y, split_type=Node.split_type)
        gini_values = k.iloc[1].values  # index 1 refers to gini values
        max_value = gini_values.max()
        index = np.where(gini_values == max_value)[0][0]

        val = k.iloc[:, [index]].loc[0]
        best_value = val.values[0]
        best_feature =val.index[0]
        return best_feature,best_value

    def has_left_child(self):
        return True if self.left_child else False

    def has_right_child(self):
        return True if self.right_child else False


def verify(x, y):
    if (isinstance(x, pd.DataFrame) or isinstance(x, pd.DataFrame)) and (
            isinstance(x, pd.DataFrame) or isinstance(y, pd.Series)):
        pass
    else:
        raise TypeError("x and y must be Pandas DataFrame")


class DecisionTree:

    def __init__(self, max_depth=None, min_infromation_gain=None):
        self.max_depth = max_depth
        self.min_info_g = min_infromation_gain
        self.root = None

    def train(self, x, y):
        verify(x, y)

        root = Node.generate_decision_node(x, y)

    def predict(self, x):
        pass

    def accuracy(self, x, y):
        pass

    def _generate_tree(self):
        pass


A = np.array([1, 2, 3])

data = pd.read_csv("../../../resources/data/500_Person_Gender_Height_Weight_Index.csv")

data['obese'] = (data.Index > 4).astype('int')
data.drop('Index', axis=1, inplace=True)
x = data.drop(['obese'], axis=1)
y = data['obese']

tree = DecisionTree()

tree.train(x, y)
